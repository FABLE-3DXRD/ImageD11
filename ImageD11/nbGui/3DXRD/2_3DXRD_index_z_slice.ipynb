{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter notebook based on ImageD11 to process 3DXRD data\n",
    "# Written by Haixing Fang, Jon Wright and James Ball\n",
    "## Date: 13/02/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have good experimental parameters, we can index more grains!\n",
    "\n",
    "For this example I have chosen a deformed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# There is a bug with the current version of ImageD11 in the site-wide Jupyter env.\n",
    "# This has been fixed here: https://github.com/FABLE-3DXRD/ImageD11/commit/4af88b886b1775585e868f2339a0eb975401468f\n",
    "# Until a new release has been made and added to the env, we need to get the latest version of ImageD11 from GitHub\n",
    "# Put it in your home directory\n",
    "# USER: Change the path below to point to your local copy of ImageD11:\n",
    "\n",
    "import os\n",
    "\n",
    "username = os.environ.get(\"USER\")\n",
    "\n",
    "id11_code_path = f\"/home/esrf/{username}/Code/ImageD11\"\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, id11_code_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import functions we need\n",
    "\n",
    "import os, glob, pprint\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib widget\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import utils\n",
    "\n",
    "import ImageD11.grain\n",
    "import ImageD11.indexing\n",
    "import ImageD11.columnfile\n",
    "from ImageD11.sinograms import properties, dataset\n",
    "\n",
    "from ImageD11.blobcorrector import eiger_spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NEW DATASETS\n",
    "\n",
    "### USER: specify your experimental directory\n",
    "\n",
    "base_dir = \"/data/visitor/ma5837/id11/20240208\"\n",
    "\n",
    "rawdata_path = os.path.join(base_dir, 'RAW_DATA')\n",
    "\n",
    "!ls -lrt {rawdata_path}\n",
    "\n",
    "processed_data_root_dir = os.path.join(base_dir, 'PROCESSED_DATA')  # USER: modify this to change the destination folder if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# USER: pick a sample and a dataset you want to segment\n",
    "\n",
    "sample = \"S12\"\n",
    "dataset = \"FF_zeries_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# desination of H5 files\n",
    "\n",
    "dset_path = os.path.join(processed_data_root_dir, sample, f\"{sample}_{dataset}\", f\"{sample}_{dataset}_dataset.h5\")\n",
    "\n",
    "# USER: specify the path to the parameter file\n",
    "\n",
    "parfile = 'Fe_refined.par'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the dataset from file\n",
    "\n",
    "ds = ImageD11.sinograms.dataset.load(dset_path)\n",
    "\n",
    "print(ds)\n",
    "print(ds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load 3d columnfile from disk\n",
    "\n",
    "cf_3d = ImageD11.columnfile.colfile_from_hdf(ds.col3dfile)\n",
    "\n",
    "cf_3d.parameters.loadparameters(parfile)\n",
    "cf_3d.updateGeometry()\n",
    "\n",
    "if \"index\" not in cf_3d.titles:\n",
    "    cf_3d.addcolumn(np.arange(cf_3d.nrows), \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the 3D peaks (fewer of them) as a cake (two-theta vs eta)\n",
    "# if the parameters in the par file are good, these should look like straight lines\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(cf_3d.ds, cf_3d.eta, s=1)\n",
    "\n",
    "ax.set_xlabel(\"D-star\")\n",
    "ax.set_ylabel(\"eta\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here we are filtering our peaks (cf_2d) to select only the strongest ones for indexing purposes only!\n",
    "# dsmax is being set to limit rings given to the indexer\n",
    "# because we have so many 3d peaks, we will filter down to 3 rings\n",
    "\n",
    "# USER: modify the \"frac\" parameter below and re-run the cell until we have around 25,000 peaks for indexing\n",
    "\n",
    "cf_strong = utils.selectpeaks(cf_3d, frac=0.6, dsmax=0.92, doplot=0.05, dstol=0.01)\n",
    "print(f\"Got {cf_strong.nrows} strong peaks for indexing\")\n",
    "cf_strong.writefile(f'{sample}_{dataset}_3d_peaks_strong.flt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we will also export some additional strong peaks across all rings\n",
    "# this will be useful for grain refinement later (using makemap)\n",
    "\n",
    "cf_strong_allrings = utils.selectpeaks(cf_3d, frac=0.85, dsmax=cf_3d.ds.max(), doplot=0.8, dstol=0.01)\n",
    "print(f\"Got {cf_strong_allrings.nrows} strong peaks for makemap\")\n",
    "cf_strong_allrings_path = f'{sample}_{dataset}_3d_peaks_strong_all_rings.flt'\n",
    "cf_strong_allrings.writefile(cf_strong_allrings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now we can take a look at the intensities of the remaining peaks\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(cf_strong.ds, cf_strong.sum_intensity,',')\n",
    "ax.semilogy()\n",
    "\n",
    "ax.set_xlabel(\"D-star\")\n",
    "ax.set_ylabel(\"Intensity\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now we can define a unit cell from our parameters\n",
    "\n",
    "Fe = ImageD11.unitcell.unitcell_from_parameters(cf_strong.parameters)\n",
    "Fe.makerings(cf_strong.ds.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now let's plot our peaks again, with the rings from the unitcell included, to check our lattice parameters are good\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "skip=1\n",
    "ax.scatter( cf_strong.ds[::skip], cf_strong.eta[::skip], s=0.5)\n",
    "ax.plot( Fe.ringds, [0,]*len(Fe.ringds), '|', ms=90, c='orange')\n",
    "ax.set_xlabel('1 / d ($\\AA$)')\n",
    "ax.set_ylabel('$\\\\eta$ (deg)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify our ImageD11 indexer with these peaks\n",
    "\n",
    "indexer = ImageD11.indexing.indexer_from_colfile(cf_strong)\n",
    "\n",
    "print(f\"Indexing {cf_strong.nrows} peaks\")\n",
    "\n",
    "# USER: set a tolerance in d-space (for assigning peaks to powder rings)\n",
    "\n",
    "indexer.ds_tol = 0.05\n",
    "\n",
    "# change the log level so we can see what the ring assigments look like\n",
    "\n",
    "ImageD11.indexing.loglevel = 1\n",
    "\n",
    "# assign peaks to powder rings\n",
    "\n",
    "indexer.assigntorings()\n",
    "\n",
    "# change log level back again\n",
    "\n",
    "ImageD11.indexing.loglevel = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# we want to index low multiplicity rings\n",
    "# choose max_multiplicity such that we get 2-3 low-multiplicity rings\n",
    "# in this case, we will find orientations on 2 rings (faster for many peaks) and include the third ring for minpeaks calculations\n",
    "\n",
    "max_multiplicity = 13\n",
    "\n",
    "min_counts_on_ring = 0\n",
    "\n",
    "n_peaks_expected = 0\n",
    "rings = []\n",
    "for i, dstar in enumerate(indexer.unitcell.ringds):\n",
    "    multiplicity = len(indexer.unitcell.ringhkls[indexer.unitcell.ringds[i]])\n",
    "    counts_on_this_ring = (indexer.ra == i).sum()\n",
    "    if counts_on_this_ring > min_counts_on_ring:\n",
    "        n_peaks_expected += multiplicity\n",
    "        if multiplicity < max_multiplicity:\n",
    "            rings.append((counts_on_this_ring, multiplicity, i))\n",
    "        \n",
    "rings.sort()\n",
    "\n",
    "print(f\"{n_peaks_expected} peaks expected\")\n",
    "print(f\"Trying these rings (counts, multiplicity, ring number): {rings}\")\n",
    "\n",
    "# USER: specify the HKL tolerances you want to use for indexing\n",
    "hkl_tols_seq = [0.01, 0.02, 0.03, 0.04, 0.05, 0.1]\n",
    "\n",
    "# USER: specify the fraction of the total expected peaks\n",
    "fracs = [0.9, 0.75]\n",
    "\n",
    "# ImageD11.cImageD11.cimaged11_omp_set_num_threads(1)\n",
    "ImageD11.indexing.loglevel=3\n",
    "\n",
    "# indexer.uniqueness = 0.3\n",
    "indexer.cosine_tol = np.cos(np.radians(90.25))\n",
    "\n",
    "# iterate over HKL tolerances\n",
    "for frac in fracs:\n",
    "    for tol in hkl_tols_seq:\n",
    "        indexer.minpks = n_peaks_expected*frac\n",
    "        indexer.hkl_tol = tol\n",
    "        \n",
    "        # iterate over rings\n",
    "        \n",
    "        for i in range(len(rings)):\n",
    "            for j in range(i, len(rings)):\n",
    "                indexer.ring_1 = rings[i][2]\n",
    "                indexer.ring_2 = rings[j][2]\n",
    "    \n",
    "                indexer.find()\n",
    "                indexer.scorethem()                \n",
    "\n",
    "        print(frac, tol, len(indexer.ubis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create grain objects\n",
    "grains = [ImageD11.grain.grain(ubi, translation=np.array([0., 0., 0.])) for ubi in indexer.ubis]\n",
    "\n",
    "# set grain GIDs (useful if we ever delete a grain)\n",
    "for i, g in enumerate(grains):\n",
    "    g.gid = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot pole figures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_ubi_path = f'{sample}_{dataset}_grains.ubi'\n",
    "tmp_map_path = f'{sample}_{dataset}_grains.map'\n",
    "\n",
    "new_flt_path = f'{sample}_{dataset}_3d_peaks_strong_all_rings.flt.new'  # flt file containing assignments from makemap\n",
    "unindexed_flt_path = f'{sample}_{dataset}_3d_peaks_strong_all_rings.flt.unindexed'  # remaining unassigned peaks from makemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ImageD11.grain.write_grain_file(tmp_ubi_path, grains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "omegas_sorted = np.sort(ds.omega)[0]\n",
    "omega_slop = np.round(np.diff(omegas_sorted).mean(), 3)\n",
    "\n",
    "makemap_hkl_tol_seq = [0.05, 0.025, 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for inc, makemap_tol in enumerate(makemap_hkl_tol_seq):\n",
    "    print(f\"Running makemap {inc+1}/{len(makemap_hkl_tol_seq)}\")\n",
    "    if inc == 0:  # ubi into map\n",
    "        makemap_output = !makemap.py -p {parfile} -u {tmp_ubi_path} -U {tmp_map_path} -f {cf_strong_allrings_path} -F {unindexed_flt_path} -s cubic -t {makemap_hkl_tol_seq[inc]} --omega_slop={omega_slop} --no_sort\n",
    "    else:  # map into map\n",
    "        makemap_output = !makemap.py -p {parfile} -u {tmp_map_path} -U {tmp_map_path} -f {cf_strong_allrings_path} -F {unindexed_flt_path} -s cubic -t {makemap_hkl_tol_seq[inc]} --omega_slop={omega_slop} --no_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# re-import our refined grains from the makemap procedure\n",
    "\n",
    "grains2 = ImageD11.grain.read_grain_file(tmp_map_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "centre_plot = False\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "xx = [grain.translation[0] for grain in grains2]\n",
    "yy = [grain.translation[1] for grain in grains2]\n",
    "zz = [grain.translation[2] for grain in grains2]\n",
    "# col = [utils.grain_to_rgb(grain) for grain in grains2]  # IPF-Z colour instead\n",
    "col = [float(grain.npks) for grain in grains2]\n",
    "sizes = [0.01*(float(grain.intensity_info.split(\"mean = \")[1].split(\" , \")[0].replace(\"'\", \"\"))) for grain in grains2]\n",
    "if centre_plot:\n",
    "    scatterplot = ax.scatter(xx-np.mean(xx), yy-np.mean(yy), zz, c=col, s=sizes)\n",
    "else:\n",
    "    scatterplot = ax.scatter(xx, yy, zz, c=col, s=sizes)\n",
    "ax.set_xlim(-200,200)\n",
    "ax.set_ylim(-200,200)\n",
    "ax.set_zlim(-200,200)\n",
    "plt.colorbar(scatterplot)\n",
    "ax.set_title(\"Grains coloured by n peaks\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_zlabel(\"z\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist([float(grain.npks) for grain in grains2], bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find the spike\n",
    "absolute_minpks = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filter out grains with fewer than 15 peaks\n",
    "grains_filtered = [grain for grain in grains2 if float(grain.npks) > absolute_minpks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "centre_plot = False\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "xx = [grain.translation[0] for grain in grains_filtered]\n",
    "yy = [grain.translation[1] for grain in grains_filtered]\n",
    "zz = [grain.translation[2] for grain in grains_filtered]\n",
    "# col = [utils.grain_to_rgb(grain) for grain in grains_filtered]  # IPF-Z colour instead\n",
    "col = [float(grain.npks) for grain in grains_filtered]\n",
    "sizes = [0.01*(float(grain.intensity_info.split(\"mean = \")[1].split(\" , \")[0].replace(\"'\", \"\"))) for grain in grains_filtered]\n",
    "if centre_plot:\n",
    "    scatterplot = ax.scatter(xx-np.mean(xx), yy-np.mean(yy), zz, c=col, s=sizes)\n",
    "else:\n",
    "    scatterplot = ax.scatter(xx, yy, zz, c=col, s=sizes)\n",
    "ax.set_xlim(-200,200)\n",
    "ax.set_ylim(-200,200)\n",
    "ax.set_zlim(-200,200)\n",
    "plt.colorbar(scatterplot)\n",
    "ax.set_title(\"Grains coloured by n peaks\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_zlabel(\"z\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for grain in grains_filtered:\n",
    "    grain.gid = int(grain.name.split(\":\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will assign our strong peaks across all rings to the grains\n",
    "# so we can save the data to file for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tol = 0.05\n",
    "\n",
    "# column to store the grain labels\n",
    "labels = np.zeros(cf_strong_allrings.nrows, 'i')\n",
    "# get all g-vectors from columnfile\n",
    "gv = np.transpose((cf_strong_allrings.gx, cf_strong_allrings.gy, cf_strong_allrings.gz)).astype(float)\n",
    "# column to store drlv2 (error in hkl)\n",
    "drlv2 =  np.ones(cf_strong_allrings.nrows, 'd')\n",
    "# iterate over all grains\n",
    "print(f\"Scoring and assigning {len(grains_filtered)} grains\")\n",
    "for g in tqdm(grains_filtered):\n",
    "    n = ImageD11.cImageD11.score_and_assign(g.ubi, gv, tol, drlv2, labels, g.gid)\n",
    "\n",
    "# add the labels column to the columnfile\n",
    "cf_strong_allrings.addcolumn(labels, 'grain_id')\n",
    "\n",
    "print(\"Storing peak data in grains\")\n",
    "# iterate through all the grains\n",
    "for g in tqdm(grains_filtered):\n",
    "    # store this grain's peak indices so we know which 4D peaks we used for indexing\n",
    "    g.peaks_3d = cf_strong_allrings.index[cf_strong_allrings.grain_id == g.gid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist([np.mean(grain.unitcell[0:3]) for grain in grains_filtered], bins=25)\n",
    "plt.show()\n",
    "\n",
    "print(np.mean([np.mean(grain.unitcell[0:3]) for grain in grains_filtered]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmp = {'compression':'gzip',\n",
    "       'compression_opts': 2,\n",
    "       'shuffle' : True }\n",
    "\n",
    "def save_array(grp, name, ary):\n",
    "    hds = grp.require_dataset(name, \n",
    "                              shape=ary.shape,\n",
    "                              dtype=ary.dtype,\n",
    "                              **cmp)\n",
    "    hds[:] = ary\n",
    "    return hds\n",
    "\n",
    "def save_grains(grains, ds):\n",
    "    with h5py.File(ds.grainsfile, 'w') as hout:\n",
    "        grn = hout.create_group('grains')\n",
    "        for g in tqdm(grains):\n",
    "            gg = grn.create_group(str(g.gid))\n",
    "            save_array(gg, 'peaks_3d_indexing', g.peaks_3d).attrs['description'] = \"Strong 3D peaks that were assigned to this grain during indexing\"\n",
    "            gg.attrs.update({'ubi':g.ubi,\n",
    "                            'translation':g.translation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save grain data\n",
    "\n",
    "save_grains(grains_filtered, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we're happy with our indexing parameters, we can define our indexing function to run for the rest of our data\n",
    "# just modify the parameters at the start of the cell below according to what you found worked well above\n",
    "# then run the two cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def index_dataset(ds):\n",
    "    cf_strong_frac = 0.6\n",
    "    cf_strong_dsmax = 0.92\n",
    "    cf_strong_dstol = 0.01\n",
    "    \n",
    "    cf_strong_allrings_frac = 0.85\n",
    "    cf_strong_allrings_dstol = 0.01\n",
    "    \n",
    "    indexer_dstol = 0.05\n",
    "    indexer_max_mult = 13\n",
    "    indexer_hkl_tols = [0.01, 0.02, 0.03, 0.04, 0.05, 0.1]\n",
    "    indexer_fracs = [0.9, 0.75]\n",
    "    indexer_cosine_tol = np.cos(np.radians(90.25))\n",
    "    indexer_max_grains = 1000\n",
    "    \n",
    "    makemap_min_ring_counts = 0\n",
    "    makemap_hkl_tol_seq = [0.05, 0.025, 0.01]\n",
    "    makemap_import_minpks = 30\n",
    "    \n",
    "    peak_assignment_hkl_tol = 0.05\n",
    "    \n",
    "    \n",
    "    print(\"Loading 3D peaks\")\n",
    "    cf_3d = ImageD11.columnfile.colfile_from_hdf(ds.col3dfile)\n",
    "    cf_3d.parameters.loadparameters(parfile)\n",
    "    cf_3d.updateGeometry()\n",
    "    if \"index\" not in cf_3d.titles:\n",
    "        cf_3d.addcolumn(np.arange(cf_3d.nrows), \"index\")\n",
    "\n",
    "    print(\"Filtering 3D peaks\")\n",
    "    cf_strong = utils.selectpeaks(cf_3d, frac=cf_strong_frac, dsmax=cf_strong_dsmax, doplot=None, dstol=cf_strong_dstol)\n",
    "    print(f\"Got {cf_strong.nrows} strong peaks for indexing\")\n",
    "    cf_strong_path = f'{sample}_{dataset}_3d_peaks_strong.flt'\n",
    "    cf_strong.writefile(cf_strong_path)\n",
    "\n",
    "    cf_strong_allrings = utils.selectpeaks(cf_3d, frac=cf_strong_allrings_frac, dsmax=cf_3d.ds.max(), doplot=None, dstol=cf_strong_allrings_dstol)\n",
    "    print(f\"Got {cf_strong_allrings.nrows} strong peaks for makemap\")\n",
    "    cf_strong_allrings_path = f'{sample}_{dataset}_3d_peaks_strong_all_rings.flt'\n",
    "    cf_strong_allrings.writefile(cf_strong_allrings_path)\n",
    "\n",
    "    print(f\"Indexing {cf_strong.nrows} peaks\")\n",
    "    Fe = ImageD11.unitcell.unitcell_from_parameters(cf_strong.parameters)\n",
    "    Fe.makerings(cf_strong.ds.max())\n",
    "    indexer = ImageD11.indexing.indexer_from_colfile(cf_strong)\n",
    "\n",
    "    ImageD11.indexing.loglevel = 3\n",
    "    \n",
    "    indexer.ds_tol = indexer_dstol\n",
    "    indexer.assigntorings()\n",
    "    indexer.max_grains = indexer_max_grains\n",
    "\n",
    "    max_multiplicity = indexer_max_mult\n",
    "    min_counts_on_ring = makemap_min_ring_counts\n",
    "\n",
    "    n_peaks_expected = 0\n",
    "    rings = []\n",
    "    for i, dstar in enumerate(indexer.unitcell.ringds):\n",
    "        multiplicity = len(indexer.unitcell.ringhkls[indexer.unitcell.ringds[i]])\n",
    "        counts_on_this_ring = (indexer.ra == i).sum()\n",
    "        if counts_on_this_ring > min_counts_on_ring:\n",
    "            n_peaks_expected += multiplicity\n",
    "            if multiplicity < max_multiplicity:\n",
    "                rings.append((counts_on_this_ring, multiplicity, i))\n",
    "\n",
    "    rings.sort()\n",
    "\n",
    "    print(f\"{n_peaks_expected} peaks expected\")\n",
    "    print(f\"Trying these rings (counts, multiplicity, ring number): {rings}\")\n",
    "    hkl_tols_seq = indexer_hkl_tols\n",
    "    fracs = indexer_fracs\n",
    "    indexer.cosine_tol = indexer_cosine_tol\n",
    "\n",
    "    for frac in fracs:\n",
    "        for tol in hkl_tols_seq:\n",
    "            indexer.minpks = n_peaks_expected*frac\n",
    "            indexer.hkl_tol = tol\n",
    "            for i in range(len(rings)):\n",
    "                for j in range(i, len(rings)):\n",
    "                    indexer.ring_1 = rings[i][2]\n",
    "                    indexer.ring_2 = rings[j][2]\n",
    "\n",
    "                    indexer.find()\n",
    "                    indexer.scorethem()\n",
    "\n",
    "\n",
    "    grains = [ImageD11.grain.grain(ubi, translation=np.array([0., 0., 0.])) for ubi in indexer.ubis]\n",
    "    print(f\"Found {len(grains)} grains\")\n",
    "\n",
    "    for i, g in enumerate(grains):\n",
    "        g.gid = i\n",
    "\n",
    "    tmp_ubi_path = f'{sample}_{dataset}_grains.ubi'\n",
    "    tmp_map_path = f'{sample}_{dataset}_grains.map'\n",
    "\n",
    "    new_flt_path = f'{sample}_{dataset}_3d_peaks_strong_all_rings.flt.new'  # flt file containing assignments from makemap\n",
    "    unindexed_flt_path = f'{sample}_{dataset}_3d_peaks_strong_all_rings.flt.unindexed'  # remaining unassigned peaks from makemap\n",
    "\n",
    "    ImageD11.grain.write_grain_file(tmp_ubi_path, grains)\n",
    "\n",
    "    omegas_sorted = np.sort(ds.omega)[0]\n",
    "    omega_slop = np.round(np.diff(omegas_sorted).mean(), 3)\n",
    "\n",
    "    makemap_hkl_tol_seq = makemap_hkl_tol_seq\n",
    "\n",
    "    for inc, makemap_tol in enumerate(makemap_hkl_tol_seq):\n",
    "        print(f\"Running makemap {inc+1}/{len(makemap_hkl_tol_seq)}\")\n",
    "        if inc == 0:  # ubi into map\n",
    "            makemap_output = !makemap.py -p {parfile} -u {tmp_ubi_path} -U {tmp_map_path} -f {cf_strong_allrings_path} -F {unindexed_flt_path} -s cubic -t {makemap_hkl_tol_seq[inc]} --omega_slop={omega_slop} --no_sort\n",
    "        else:  # map into map\n",
    "            makemap_output = !makemap.py -p {parfile} -u {tmp_map_path} -U {tmp_map_path} -f {cf_strong_allrings_path} -F {unindexed_flt_path} -s cubic -t {makemap_hkl_tol_seq[inc]} --omega_slop={omega_slop} --no_sort\n",
    "\n",
    "    grains2 = ImageD11.grain.read_grain_file(tmp_map_path)\n",
    "    absolute_minpks = makemap_import_minpks\n",
    "    grains_filtered = [grain for grain in grains2 if float(grain.npks) > absolute_minpks]\n",
    "\n",
    "    for grain in grains_filtered:\n",
    "        grain.gid = int(grain.name.split(\":\")[0])\n",
    "\n",
    "    tol = peak_assignment_hkl_tol\n",
    "\n",
    "    labels = np.zeros(cf_strong_allrings.nrows, 'i')\n",
    "    gv = np.transpose((cf_strong_allrings.gx, cf_strong_allrings.gy, cf_strong_allrings.gz)).astype(float)\n",
    "    drlv2 =  np.ones(cf_strong_allrings.nrows, 'd')\n",
    "    print(f\"Scoring and assigning {len(grains_filtered)} grains\")\n",
    "    for g in tqdm(grains_filtered):\n",
    "        n = ImageD11.cImageD11.score_and_assign(g.ubi, gv, tol, drlv2, labels, g.gid)\n",
    "\n",
    "    cf_strong_allrings.addcolumn(labels, 'grain_id')\n",
    "\n",
    "    print(\"Storing peak data in grains\")\n",
    "    for g in tqdm(grains_filtered):\n",
    "        g.peaks_3d = cf_strong_allrings.index[cf_strong_allrings.grain_id == g.gid]\n",
    "    \n",
    "    print(\"Saving grains\")\n",
    "    save_grains(grains_filtered, ds)\n",
    "    \n",
    "    if os.path.exists(cf_strong_path):\n",
    "        os.remove(cf_strong_path)\n",
    "        \n",
    "    if os.path.exists(cf_strong_allrings_path):\n",
    "        os.remove(cf_strong_allrings_path)\n",
    "    \n",
    "    if os.path.exists(tmp_ubi_path):\n",
    "        os.remove(tmp_ubi_path)\n",
    "        \n",
    "    if os.path.exists(tmp_map_path):\n",
    "        os.remove(tmp_map_path)\n",
    "        \n",
    "    if os.path.exists(new_flt_path):\n",
    "        os.remove(new_flt_path)\n",
    "        \n",
    "    if os.path.exists(unindexed_flt_path):\n",
    "        os.remove(unindexed_flt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we're happy with our indexing parameters, we can run the below cell to do this in bulk for many samples/datasets\n",
    "# just modify samples_dict accordingly!\n",
    "\n",
    "parfile = 'Fe_refined.par'\n",
    "\n",
    "samples_dict = {\n",
    "    \"S13\" : [\n",
    "        \"FF_zeries_0\",\n",
    "        \"FF_zeries_1\",\n",
    "        \"FF_zeries_2\",\n",
    "        \"FF_zeries_3\",\n",
    "        \"FF_zeries_4\",\n",
    "    ],\n",
    "    \"S14\" : [\n",
    "        \"FF_zeries_0\",\n",
    "        \"FF_zeries_1\",\n",
    "        \"FF_zeries_2\",\n",
    "        \"FF_zeries_3\",\n",
    "        \"FF_zeries_4\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "for sample, datasets in samples_dict.items():\n",
    "    for dataset in datasets:\n",
    "        print(f\"Processing dataset {dataset} in sample {sample}\")\n",
    "        print(\"Importing DataSet object\")\n",
    "        dset_path = os.path.join(processed_data_root_dir, sample, f\"{sample}_{dataset}\", f\"{sample}_{dataset}_dataset.h5\")\n",
    "        ds = ImageD11.sinograms.dataset.load(dset_path)\n",
    "        print(f\"I have a DataSet {ds.dset} in sample {ds.sample}\")\n",
    "        \n",
    "        index_dataset(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (main)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
