{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Jupyter notebook based on ImageD11 to process scanning 3DXRD data\n",
    "# Written by Haixing Fang, Jon Wright and James Ball\n",
    "## Date: 12/10/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will try to perform a point-by-point index of the 2D peaks you segmented.\n",
    "The point-by-point results (usually saved to a .txt file in the sample PROCESSED_DATA folder) are multi-valued (we can find multiple UBIs at each map voxel).  \n",
    "You can view the results of the point-by-point process 'live' by running the next notebook (pbp_2_visualise).  \n",
    "That notebook will also allow you to save a single-valued version of the pbp map to H5, ParaView XDMF and MTEX CTF.\n",
    "The UBIs we find from the PBP index should have reasonably accurate orientations, but the strains are likely to be poor.  \n",
    "To get much better strains, slightly better orientations and possibly better grain shapes, you should run pbp_3_refinement\n",
    "Then run 4_visualise to convert the refinement results to an accurate single-valued map with good strains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exec(open('/data/id11/nanoscope/install_ImageD11_from_git.py').read())\n",
    "PYTHONPATH = setup_ImageD11_from_git( ) # ( os.path.join( os.environ['HOME'],'Code'), 'ImageD11_git' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib ipympl\n",
    "\n",
    "import ImageD11.sinograms.point_by_point\n",
    "import ImageD11.sinograms.dataset\n",
    "import ImageD11.sinograms.properties\n",
    "import ImageD11.columnfile\n",
    "\n",
    "import ImageD11.nbGui.nb_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# USER: Pass path to dataset file\n",
    "\n",
    "dset_file = 'si_cube_test/processed/Si_cube/Si_cube_S3DXRD_nt_moves_dty/Si_cube_S3DXRD_nt_moves_dty_dataset.h5'\n",
    "\n",
    "ds = ImageD11.sinograms.dataset.load(dset_file)\n",
    "   \n",
    "sample = ds.sample\n",
    "dataset = ds.dsname\n",
    "rawdata_path = ds.dataroot\n",
    "processed_data_root_dir = ds.analysisroot\n",
    "\n",
    "print(ds)\n",
    "print(ds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# USER: specify the path to the parameter file\n",
    "\n",
    "par_file = os.path.join(processed_data_root_dir, 'pars.json')\n",
    "\n",
    "# add them to the dataset\n",
    "\n",
    "ds.parfile = par_file\n",
    "\n",
    "ds.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load phases from parameter file\n",
    "\n",
    "ds.phases = ds.get_phases_from_disk()\n",
    "ds.phases.unitcells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now let's select a phase to index from our parameters json\n",
    "phase_str = 'Si'\n",
    "\n",
    "ucell = ds.phases.unitcells[phase_str]\n",
    "\n",
    "print(ucell.lattice_parameters, ucell.spacegroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will now generate a cf (columnfile) object for the 2D peaks.\n",
    "# Will be corrected for detector spatial distortion\n",
    "\n",
    "cf_2d = ds.get_cf_2d()\n",
    "ds.update_colfile_pars(cf_2d, phase_name=phase_str)\n",
    "\n",
    "if not os.path.exists(ds.col2dfile):\n",
    "    # save the 4D peaks to file so we don't have to spatially correct them again\n",
    "    ImageD11.columnfile.colfile_to_hdf(cf_2d, ds.col2dfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filter the columnfile to discard weak peaks\n",
    "\n",
    "minpkint = 5\n",
    "\n",
    "cf_2d.filter(cf_2d.Number_of_pixels > minpkint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pbp_object = ImageD11.sinograms.point_by_point.PBP(ds.parfile,\n",
    "                                                    ds,\n",
    "                                                    hkl_tol=0.025,\n",
    "                                                    fpks=0.9,\n",
    "                                                    ds_tol=0.004,\n",
    "                                                    etacut=0.1,\n",
    "                                                    ifrac=5e-3,\n",
    "                                                    cosine_tol=np.cos(np.radians(90 - ds.ostep)),\n",
    "                                                    y0=0.0,\n",
    "                                                    symmetry=\"cubic\",\n",
    "                                                    foridx=[0, 1, 3, 5, 7],\n",
    "                                                    forgen=[1, 5, 7],\n",
    "                                                    uniqcut=0.85,\n",
    "                                                    phase_name=phase_str)\n",
    "\n",
    "pbp_object.setpeaks(cf_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = pbp_object.iplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_cluster = True\n",
    "\n",
    "if use_cluster:\n",
    "    bash_script_path = utils.prepare_pbp_bash(pbp_object, PYTHONPATH, minpkint)\n",
    "    utils.slurm_submit_and_wait(bash_script_path, 15)\n",
    "else:\n",
    "    pbp_object.point_by_point(ds.pbpfile, loglevel=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    raise ValueError(\"Change the 1 above to 0 to allow 'Run all cells' in the notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now that we're happy with our indexing parameters, we can run the below cell to do this in bulk for many samples/datasets\n",
    "# by default this will do all samples in sample_list, all datasets with a prefix of dset_prefix\n",
    "# you can add samples and datasets to skip in skips_dict\n",
    "\n",
    "skips_dict = {\n",
    "    \"FeAu_0p5_tR_nscope\": [\"top_-50um\", \"top_-100um\"]\n",
    "}\n",
    "\n",
    "dset_prefix = \"top\"\n",
    "\n",
    "sample_list = [\"FeAu_0p5_tR_nscope\"]\n",
    "    \n",
    "samples_dict = utils.find_datasets_to_process(ds.dataroot, skips_dict, dset_prefix, sample_list)\n",
    "    \n",
    "# manual override:\n",
    "# samples_dict = {\"FeAu_0p5_tR_nscope\": [\"top_100um\", \"top_150um\"]}\n",
    "    \n",
    "# now we have our samples_dict, we can process our data:\n",
    "\n",
    "first_pbp_object = pbp_object\n",
    "\n",
    "sbats = []\n",
    "for sample, datasets in samples_dict.items():\n",
    "    for dataset in datasets:\n",
    "        print(f\"Processing dataset {dataset} in sample {sample}\")\n",
    "        dset_path = os.path.join(ds.analysisroot, sample, f\"{sample}_{dataset}\", f\"{sample}_{dataset}_dataset.h5\")\n",
    "        if not os.path.exists(dset_path):\n",
    "            print(f\"Missing DataSet file for {dataset} in sample {sample}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        print(\"Importing DataSet object\")\n",
    "        \n",
    "        ds = ImageD11.sinograms.dataset.load(dset_path)\n",
    "        print(f\"I have a DataSet {ds.dset} in sample {ds.sample}\")\n",
    "        if os.path.exists(ds.pbpfile):\n",
    "            print(f\"Already have PBP file for {dataset} in sample {sample}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        ds.parfile = par_file\n",
    "        ds.save()\n",
    "        \n",
    "        ds.phases = ds.get_phases_from_disk()\n",
    "        \n",
    "        cf_2d = ds.get_cf_2d()\n",
    "        ds.update_colfile_pars(cf_2d, phase_name=phase_str)\n",
    "\n",
    "        if not os.path.exists(ds.col2dfile):\n",
    "            ImageD11.columnfile.colfile_to_hdf(cf_2d, ds.col2dfile)\n",
    "            \n",
    "        cf_2d.filter(cf_2d.Number_of_pixels > minpkint)\n",
    "        \n",
    "        pbp_object = ImageD11.sinograms.point_by_point.PBP(ds.parfile,\n",
    "                                                            ds,\n",
    "                                                            hkl_tol=first_pbp_object.hkl_tol,\n",
    "                                                            fpks=first_pbp_object.fpks,\n",
    "                                                            ds_tol=first_pbp_object.ds_tol,\n",
    "                                                            etacut=first_pbp_object.etacut,\n",
    "                                                            ifrac=first_pbp_object.ifrac,\n",
    "                                                            cosine_tol=first_pbp_object.cosine_tol,\n",
    "                                                            y0=first_pbp_object.y0,\n",
    "                                                            symmetry=first_pbp_object.symmetry,\n",
    "                                                            foridx=first_pbp_object.foridx,\n",
    "                                                            forgen=first_pbp_object.forgen,\n",
    "                                                            uniqcut=first_pbp_object.uniqcut,\n",
    "                                                              phase_name=first_pbp_object.phase_name)\n",
    "        \n",
    "        pbp_object.setpeaks(cf_2d)\n",
    "\n",
    "        if use_cluster:\n",
    "            # get the sbat and submit them all at once\n",
    "            bash_script_path = utils.prepare_pbp_bash(pbp_object, PYTHONPATH, minpkint)\n",
    "            sbats.append(bash_script_path)\n",
    "            # utils.slurm_submit_and_wait(bash_script_path, 15)\n",
    "        else:\n",
    "            # do it locally\n",
    "            pbp_object.point_by_point(ds.pbpfile, loglevel=3)\n",
    "        \n",
    "        ds.save()\n",
    "    if use_cluster:\n",
    "        utils.slurm_submit_many_and_wait(sbats, wait_time_sec=60)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (main)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
