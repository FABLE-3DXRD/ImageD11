{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter notebook based on ImageD11 to process scanning 3DXRD data\n",
    "# Written by Haixing Fang, Jon Wright and James Ball\n",
    "## Date: 26/02/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# USER: Change the path below to point to your local copy of ImageD11:\n",
    "\n",
    "import os\n",
    "\n",
    "home_dir = !echo $HOME\n",
    "home_dir = str(home_dir[0])\n",
    "\n",
    "# USER: You can change this location if you want\n",
    "\n",
    "id11_code_path = os.path.join(home_dir, \"Code/ImageD11\")\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, id11_code_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import functions we need\n",
    "\n",
    "import glob, pprint\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib ipympl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import ImageD11.nbGui.nb_utils as utils\n",
    "\n",
    "import ImageD11.grain\n",
    "import ImageD11.indexing\n",
    "import ImageD11.columnfile\n",
    "from ImageD11.sinograms import properties, dataset\n",
    "\n",
    "from ImageD11.blobcorrector import eiger_spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE: For old datasets before the new directory layout structure, we don't distinguish between RAW_DATA and PROCESSED_DATA\n",
    "\n",
    "### USER: specify your experimental directory\n",
    "\n",
    "rawdata_path = \"/data/visitor/ihma439/id11/20231211/RAW_DATA\"\n",
    "\n",
    "!ls -lrt {rawdata_path}\n",
    "\n",
    "### USER: specify where you want your processed data to go\n",
    "\n",
    "processed_data_root_dir = \"/data/visitor/ihma439/id11/20231211/PROCESSED_DATA/James/nb_testing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# USER: pick a sample and a dataset you want to segment\n",
    "\n",
    "sample = \"FeAu_0p5_tR_nscope\"\n",
    "dataset = \"top_250um\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# desination of H5 files\n",
    "\n",
    "dset_file = os.path.join(processed_data_root_dir, sample, f\"{sample}_{dataset}\", f\"{sample}_{dataset}_dataset.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the dataset from file\n",
    "\n",
    "ds = ImageD11.sinograms.dataset.load(dset_file)\n",
    "\n",
    "print(ds)\n",
    "print(ds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# USER: specify the path to the parameter file and spatial distortion files\n",
    "\n",
    "par_file = os.path.join(processed_data_root_dir, '../../../SCRIPTS/James/S3DXRD/Fe_refined.par')\n",
    "e2dx_file = os.path.join(processed_data_root_dir, '../../CeO2/e2dx_E-08-0173_20231127.edf')\n",
    "e2dy_file = os.path.join(processed_data_root_dir, '../../CeO2/e2dy_E-08-0173_20231127.edf')\n",
    "\n",
    "# add them to the dataset\n",
    "\n",
    "ds.parfile = par_file\n",
    "ds.e2dxfile = e2dx_file\n",
    "ds.e2dyfile = e2dy_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we already have the 2D and 4D peaks, we could just load them from file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cf_2d = ImageD11.columnfile.colfile_from_hdf(ds.col2dfile)\n",
    "# cf_2d.parameters.loadparameters(ds.par_file)\n",
    "# cf_2d.updateGeometry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cf_4d = ImageD11.columnfile.colfile_from_hdf(ds.col4dfile)\n",
    "# cf_4d.parameters.loadparameters(ds.par_file)\n",
    "# cf_4d.updateGeometry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# otherwise load them from the peaks table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import 2D peaks, make a spatially corrected columnfile, save it\n",
    "\n",
    "peaks_table = ImageD11.sinograms.properties.pks_table.load(ds.pksfile)\n",
    "\n",
    "# Grab the 2d peak centroids\n",
    "peaks_2d = peaks_table.pk2d(ds.omega, ds.dty)\n",
    "cf_2d = utils.tocolf(peaks_2d, ds)\n",
    "\n",
    "if os.path.exists(ds.col2dfile):\n",
    "    os.remove(ds.col2dfile)\n",
    "\n",
    "# save the 2D peaks to file so we don't have to spatially correct them again\n",
    "ImageD11.columnfile.colfile_to_hdf(cf_2d, ds.col2dfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will now generate a cf (columnfile) object for the 4D peaks.\n",
    "# Will be corrected for detector spatial distortion\n",
    "\n",
    "peaks_4d = peaks_table.pk2dmerge(ds.omega, ds.dty)\n",
    "cf_4d = utils.tocolf(peaks_4d, ds)  # spatial correction\n",
    "\n",
    "# uncomment below if you don't want spatial correction for some reason\n",
    "# cf_4d = ImageD11.columnfile.colfile_from_dict(peaks_4d)\n",
    "# cf_4d.addcolumn(cf_4d.s_raw, \"sc\")\n",
    "# cf_4d.addcolumn(cf_4d.f_raw, \"fc\")\n",
    "# cf_4d.parameters.loadparameters(ds.par_file)\n",
    "# cf_4d.updateGeometry()\n",
    "\n",
    "# the first thing we should do is create an index column for our 4D peaks\n",
    "index_column = np.arange(cf_4d.nrows)\n",
    "cf_4d.addcolumn(index_column, 'index')\n",
    "\n",
    "# Delete the columnfile output file if it exists\n",
    "\n",
    "if os.path.exists(ds.col4dfile):\n",
    "    os.remove(ds.col4dfile)\n",
    "    \n",
    "# save the 4D peaks to file so we don't have to spatially correct them again\n",
    "ImageD11.columnfile.colfile_to_hdf(cf_4d, ds.col4dfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate a mask that selects only 4D peaks greater than 25 pixels in size\n",
    "\n",
    "m = cf_4d['Number_of_pixels'] > 25\n",
    "\n",
    "# then plot omega vs dty for all peaks - should look sinusoidal\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "counts, xedges, yedges, im = ax.hist2d(cf_4d['omega'][m], cf_4d['dty'][m], weights=np.sqrt(cf_4d['sum_intensity'][m]), bins=(ds.obinedges, ds.ybinedges), norm=matplotlib.colors.LogNorm())\n",
    "ax.set_xlabel(\"Omega angle\")\n",
    "ax.set_ylabel(\"dty\")\n",
    "\n",
    "fig.colorbar(im, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the 4D peaks (fewer of them) as a cake (two-theta vs eta)\n",
    "# if the parameters in the par file are good, these should look like straight lines\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(cf_4d.ds, cf_4d.eta, s=1)\n",
    "\n",
    "ax.set_xlabel(\"dstar\")\n",
    "ax.set_ylabel(\"eta\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: export CF to an flt so we can play with it with ImageD11_gui\n",
    "# uncomment the below line\n",
    "\n",
    "# cf_4d.writefile(f'{sample}_{dataset}_4d_peaks.flt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here we are filtering our peaks (cf_4d) to select only the strongest ones for indexing purposes only!\n",
    "# dsmax is being set to limit rings given to the indexer - 6-8 rings is normally good\n",
    "\n",
    "# USER: modify the \"frac\" parameter below and re-run the cell until the orange dot sits nicely on the \"elbow\" of the blue line\n",
    "# this indicates the fractional intensity cutoff we will select\n",
    "# if the blue line does not look elbow-shaped in the logscale plot, try changing the \"doplot\" parameter (the y scale of the logscale plot) until it does\n",
    "\n",
    "cf_strong_frac = 0.994\n",
    "cf_strong_dsmax = 1.155\n",
    "cf_strong_dstol = 0.005\n",
    "\n",
    "cf_strong = utils.selectpeaks(cf_4d, frac=cf_strong_frac, dsmax=cf_strong_dsmax, dstol=cf_strong_dstol, doplot=0.95)\n",
    "print(cf_4d.nrows)\n",
    "print(cf_strong.nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: export CF to an flt so we can play with it with ImageD11_gui\n",
    "# uncomment the below line\n",
    "\n",
    "# cf_strong.writefile(f'{sample}_{dataset}_strong_4d_peaks.flt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now we can take a look at the intensities of the remaining peaks\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(cf_strong.ds, cf_strong.sum_intensity,',')\n",
    "ax.semilogy()\n",
    "\n",
    "ax.set_xlabel(\"Dstar\")\n",
    "ax.set_ylabel(\"Intensity\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now we can define a unit cell from our parameters\n",
    "\n",
    "ucell = ImageD11.unitcell.unitcell_from_parameters(cf_strong.parameters)\n",
    "ucell.makerings(cf_strong.ds.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now let's plot our peaks again, with the rings from the unitcell included, to check our lattice parameters are good\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "skip=1\n",
    "ax.scatter( cf_strong.ds[::skip], cf_strong.eta[::skip], s=0.5)\n",
    "ax.plot( ucell.ringds, [0,]*len(ucell.ringds), '|', ms=90, c=\"red\")\n",
    "ax.set_xlabel('1 / d ($\\AA$)')\n",
    "ax.set_ylabel('$\\\\eta$ (deg)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify our ImageD11 indexer with these peaks\n",
    "# we're aiming to index around 3_000 to 10_000 peaks\n",
    "\n",
    "indexer = ImageD11.indexing.indexer_from_colfile(cf_strong)\n",
    "\n",
    "print(f\"Indexing {cf_strong.nrows} peaks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# USER: set a tolerance in d-space (for assigning peaks to powder rings)\n",
    "\n",
    "indexer_ds_tol = 0.01\n",
    "indexer.ds_tol = indexer_ds_tol\n",
    "\n",
    "# change the log level so we can see what the ring assigments look like\n",
    "\n",
    "ImageD11.indexing.loglevel = 1\n",
    "\n",
    "# assign peaks to powder rings\n",
    "\n",
    "indexer.assigntorings()\n",
    "\n",
    "# change log level back again\n",
    "\n",
    "ImageD11.indexing.loglevel = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's plot the assigned peaks\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# indexer.ra is the ring assignments\n",
    "\n",
    "ax.scatter(cf_strong.ds, cf_strong.eta, c=indexer.ra, cmap='tab20', s=1)\n",
    "ax.set_xlabel(\"d-star\")\n",
    "ax.set_ylabel(\"eta\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now we are indexing!\n",
    "# we have to choose which rings we want to generate orientations on\n",
    "# generally we want two or three low-multiplicity rings that are isolated from other phases\n",
    "# take a look at the ring assignment output from a few cells above, and choose two or three\n",
    "rings_for_gen = [0, 1, 3]\n",
    "\n",
    "# now we want to decide which rings to score our found orientations against\n",
    "# generally we can just exclude dodgy rings (close to other phases, only a few peaks in etc)\n",
    "rings_for_scoring = [0, 1, 2, 3, 4]\n",
    "\n",
    "# the sequence of hkl tolerances the indexer will iterate through\n",
    "hkl_tols_seq = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.075]\n",
    "# the sequence of minpks fractions the indexer will iterate through\n",
    "fracs = [0.9, 0.7]\n",
    "# the tolerance in g-vector angle\n",
    "cosine_tol = np.cos(np.radians(90 - 0.25))\n",
    "# the max number of UBIs we can find per pair of rings\n",
    "max_grains = 1000\n",
    "\n",
    "grains, indexer = utils.do_index(cf=cf_strong,\n",
    "                                dstol=indexer_ds_tol,\n",
    "                                forgen=rings_for_gen,\n",
    "                                foridx=rings_for_scoring,\n",
    "                                hkl_tols=hkl_tols_seq,\n",
    "                                fracs=fracs,\n",
    "                                cosine_tol=cosine_tol,\n",
    "                                max_grains=max_grains\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set grain GIDs (useful if we ever delete a grain)\n",
    "for i, g in enumerate(grains):\n",
    "    g.gid = i\n",
    "    \n",
    "    g.a = np.cbrt(np.linalg.det(g.ubi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_unit_cell_lengths = [grain.a for grain in grains]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(mean_unit_cell_lengths)\n",
    "ax.set_xlabel(\"Grain ID\")\n",
    "ax.set_ylabel(\"Unit cell length\")\n",
    "plt.show()\n",
    "\n",
    "a0 = np.median(mean_unit_cell_lengths)\n",
    "    \n",
    "print(a0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# assign peaks to grains\n",
    "\n",
    "peak_assign_tol = 0.05\n",
    "\n",
    "utils.assign_peaks_to_grains(grains, cf_strong, tol=peak_assign_tol)\n",
    "\n",
    "print(\"Storing peak data in grains\")\n",
    "# iterate through all the grains\n",
    "for g in tqdm(grains):\n",
    "    # store this grain's peak indices so we know which 4D peaks we used for indexing\n",
    "    g.mask_4d = cf_strong.grain_id == g.gid\n",
    "    g.peaks_4d = cf_strong.index[cf_strong.grain_id == g.gid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.plot_index_results(indexer, cf_strong, 'First attempt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.plot_grain_sinograms(grains, cf_strong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save grain data\n",
    "\n",
    "utils.save_s3dxrd_grains_after_indexing(grains, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save new things to the dataset\n",
    "\n",
    "ds.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    raise ValueError(\"Change the 1 above to 0 to allow 'Run all cells' in the notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now that we're happy with our indexing parameters, we can run the below cell to do this in bulk for many samples/datasets\n",
    "# by default this will do all samples in sample_list, all datasets with a prefix of dset_prefix\n",
    "# you can add samples and datasets to skip in skips_dict\n",
    "\n",
    "skips_dict = {\n",
    "    \"FeAu_0p5_tR_nscope\": [\"top_-50um\", \"top_-100um\"]\n",
    "}\n",
    "\n",
    "dset_prefix = \"top\"\n",
    "\n",
    "sample_list = [\"FeAu_0p5_tR_nscope\"]\n",
    "    \n",
    "samples_dict = utils.find_datasets_to_process(rawdata_path, skips_dict, dset_prefix, sample_list)\n",
    "    \n",
    "# manual override:\n",
    "# samples_dict = {\"FeAu_0p5_tR_nscope\": [\"top_100um\", \"top_200um\"]}\n",
    "    \n",
    "# now we have our samples_dict, we can process our data:\n",
    "\n",
    "\n",
    "for sample, datasets in samples_dict.items():\n",
    "    for dataset in datasets:\n",
    "        print(f\"Processing dataset {dataset} in sample {sample}\")\n",
    "        dset_path = os.path.join(processed_data_root_dir, sample, f\"{sample}_{dataset}\", f\"{sample}_{dataset}_dataset.h5\")\n",
    "        if not os.path.exists(dset_path):\n",
    "            print(f\"Missing DataSet file for {dataset} in sample {sample}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        print(\"Importing DataSet object\")\n",
    "        \n",
    "        ds = ImageD11.sinograms.dataset.load(dset_path)\n",
    "        print(f\"I have a DataSet {ds.dset} in sample {ds.sample}\")\n",
    "        if os.path.exists(ds.grainsfile):\n",
    "            print(f\"Already have grains for {dataset} in sample {sample}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        ds.parfile = par_file\n",
    "        ds.e2dxfile = e2dx_file\n",
    "        ds.e2dyfile = e2dy_file\n",
    "        \n",
    "        peaks_table = ImageD11.sinograms.properties.pks_table.load(ds.pksfile)\n",
    "        peaks_2d = peaks_table.pk2d(ds.omega, ds.dty)\n",
    "        cf_2d = utils.tocolf(peaks_2d, ds)\n",
    "        if os.path.exists(ds.col2dfile):\n",
    "            os.remove(ds.col2dfile)\n",
    "        ImageD11.columnfile.colfile_to_hdf(cf_2d, ds.col2dfile)\n",
    "\n",
    "        peaks_4d = peaks_table.pk2dmerge(ds.omega, ds.dty)\n",
    "        cf_4d = utils.tocolf(peaks_4d, ds)  # spatial correction\n",
    "        index_column = np.arange(cf_4d.nrows)\n",
    "        cf_4d.addcolumn(index_column, 'index')\n",
    "        if os.path.exists(ds.col4dfile):\n",
    "            os.remove(ds.col4dfile)\n",
    "        ImageD11.columnfile.colfile_to_hdf(cf_4d, ds.col4dfile)\n",
    "        \n",
    "        cf_strong = utils.selectpeaks(cf_4d, frac=cf_strong_frac, dsmax=cf_strong_dsmax, dstol=cf_strong_dstol)\n",
    "\n",
    "        grains, indexer = utils.do_index(cf=cf_strong,\n",
    "                                        dstol=indexer_ds_tol,\n",
    "                                        forgen=rings_for_gen,\n",
    "                                        foridx=rings_for_scoring,\n",
    "                                        hkl_tols=hkl_tols_seq,\n",
    "                                        fracs=fracs,\n",
    "                                        cosine_tol=cosine_tol,\n",
    "                                        max_grains=max_grains\n",
    "        )\n",
    "        \n",
    "        for i, g in enumerate(grains):\n",
    "            g.gid = i\n",
    "            \n",
    "        utils.assign_peaks_to_grains(grains, cf_strong, tol=peak_assign_tol)\n",
    "\n",
    "        print(\"Storing peak data in grains\")\n",
    "        for g in tqdm(grains):\n",
    "            g.mask_4d = cf_strong.grain_id == g.gid\n",
    "            g.peaks_4d = cf_strong.index[cf_strong.grain_id == g.gid]\n",
    "            \n",
    "        utils.save_s3dxrd_grains_after_indexing(grains, ds)\n",
    "        \n",
    "        ds.save()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (main)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
