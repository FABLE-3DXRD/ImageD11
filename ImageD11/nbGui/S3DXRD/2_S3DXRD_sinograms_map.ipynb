{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter notebook based on ImageD11 to process scanning 3DXRD data\n",
    "# Written by Haixing Fang, Jon Wright and James Ball\n",
    "## Date: 26/02/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# There is a bug with the current version of ImageD11 in the site-wide Jupyter env.\n",
    "# This has been fixed here: https://github.com/FABLE-3DXRD/ImageD11/commit/4af88b886b1775585e868f2339a0eb975401468f\n",
    "# Until a new release has been made and added to the env, we need to get the latest version of ImageD11 from GitHub\n",
    "# Put it in your home directory\n",
    "# USER: Change the path below to point to your local copy of ImageD11:\n",
    "\n",
    "import os\n",
    "\n",
    "username = os.environ.get(\"USER\")\n",
    "\n",
    "id11_code_path = f\"/home/esrf/{username}/Code/ImageD11\"\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, id11_code_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import functions we need\n",
    "\n",
    "import concurrent.futures\n",
    "import timeit\n",
    "import glob\n",
    "import pprint\n",
    "from shutil import rmtree\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib ipympl\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import h5py\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ImageD11.columnfile\n",
    "from ImageD11.sinograms import properties, roi_iradon\n",
    "from ImageD11.blobcorrector import eiger_spatial\n",
    "from ImageD11.grain import grain\n",
    "\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.morphology import convex_hull_image\n",
    "\n",
    "import ImageD11.nbGui.nb_utils as utils\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define our functions\n",
    "\n",
    "def read_grains(ds):\n",
    "    with h5py.File(ds.grainsfile, 'r') as hin:      \n",
    "        grains_group = 'grains'\n",
    "        \n",
    "        grains = []\n",
    "        for gid_string in tqdm(sorted(hin[grains_group].keys(), key=lambda x: int(x))):\n",
    "            gg = hin[grains_group][gid_string]\n",
    "            ubi = gg.attrs['ubi'][:]\n",
    "            g = ImageD11.grain.grain(ubi)\n",
    "            g.gid = int(gid_string)\n",
    "            grains.append(g)\n",
    "    \n",
    "    return grains\n",
    "\n",
    "\n",
    "def map_grain_from_peaks(g, flt, ds):\n",
    "    \"\"\"\n",
    "    Computes sinogram\n",
    "    flt is already the peaks for this grain\n",
    "    Returns angles, sino\n",
    "    \"\"\"   \n",
    "    NY = len(ds.ybincens)  # number of y translations\n",
    "    iy = np.round((flt.dty - ds.ybincens[0]) / (ds.ybincens[1]-ds.ybincens[0])).astype(int)  # flt column for y translation index\n",
    "\n",
    "    # The problem is to assign each spot to a place in the sinogram\n",
    "    hklmin = g.hkl_2d_strong.min(axis=1)  # Get minimum integer hkl (e.g -10, -9, -10)\n",
    "    dh = g.hkl_2d_strong - hklmin[:,np.newaxis]  # subtract minimum hkl from all integer hkls\n",
    "    de = (g.etasigns_2d_strong.astype(int) + 1)//2  # something signs related\n",
    "    #   4D array of h,k,l,+/-\n",
    "    # pkmsk is whether a peak has been observed with this HKL or not\n",
    "    pkmsk = np.zeros(list(dh.max(axis=1) + 1 )+[2,], int)  # make zeros-array the size of (max dh +1) and add another axis of length 2\n",
    "    pkmsk[ dh[0], dh[1], dh[2], de ] = 1  # we found these HKLs for this grain\n",
    "    #   sinogram row to hit\n",
    "    pkrow = np.cumsum(pkmsk.ravel()).reshape(pkmsk.shape) - 1  #\n",
    "    # counting where we hit an HKL position with a found peak\n",
    "    # e.g (-10, -9, -10) didn't get hit, but the next one did, so increment\n",
    "\n",
    "    npks = pkmsk.sum( )\n",
    "    destRow = pkrow[ dh[0], dh[1], dh[2], de ] \n",
    "    sino = np.zeros( ( npks, NY ), 'f' )\n",
    "    hits = np.zeros( ( npks, NY ), 'f' )\n",
    "    angs = np.zeros( ( npks, NY ), 'f' )\n",
    "    adr = destRow * NY + iy \n",
    "    # Just accumulate \n",
    "    sig = flt.sum_intensity\n",
    "    ImageD11.cImageD11.put_incr64( sino, adr, sig )\n",
    "    ImageD11.cImageD11.put_incr64( hits, adr, np.ones(len(de),dtype='f'))\n",
    "    ImageD11.cImageD11.put_incr64( angs, adr, flt.omega)\n",
    "    \n",
    "    sinoangles = angs.sum( axis = 1) / hits.sum( axis = 1 )\n",
    "    # Normalise:\n",
    "    sino = (sino.T/sino.max( axis=1 )).T\n",
    "    # Sort (cosmetic):\n",
    "    order = np.lexsort((np.arange(npks), sinoangles))\n",
    "    sinoangles = sinoangles[order]\n",
    "    ssino = sino[order].T\n",
    "    return sinoangles, ssino, hits[order].T\n",
    "\n",
    "def do_sinos(g, hkltol=0.25):\n",
    "    flt = utils.tocolf({p:p2d[p][g.peaks_2d] for p in p2d}, par_path, dxfile=e2dx_path, dyfile=e2dy_path)  # convert it to a columnfile and spatially correct\n",
    "    \n",
    "    hkl_real = np.dot(g.ubi, (flt.gx, flt.gy, flt.gz))  # calculate hkl of all assigned peaks\n",
    "    hkl_int = np.round(hkl_real).astype(int) # round to nearest integer\n",
    "    dh = ((hkl_real - hkl_int)**2).sum(axis = 0)  # calculate square of difference\n",
    "\n",
    "    # g.dherrall = dh.mean()  # mean hkl error across all assigned peaks\n",
    "    # g.npksall = flt.nrows  # total number of assigned peaks\n",
    "    flt.filter(dh < hkltol*hkltol)  # filter all assigned peaks to be less than hkltol squared\n",
    "    hkl_real = np.dot(g.ubi, (flt.gx, flt.gy, flt.gz))  # recalculate error after filtration\n",
    "    hkl_int = np.round(hkl_real).astype(int)\n",
    "    dh = ((hkl_real - hkl_int)**2).sum(axis = 0)\n",
    "    # g.dherr = dh.mean()  # dherr is mean hkl error across assigned peaks after hkltol filtering\n",
    "    # g.npks = flt.nrows  # total number of assigned peaks after hkltol filtering\n",
    "    g.etasigns_2d_strong = np.sign(flt.eta)\n",
    "    g.hkl_2d_strong = hkl_int  # integer hkl of assigned peaks after hkltol filtering\n",
    "    g.sinoangles, g.ssino, g.hits = map_grain_from_peaks(g, flt, ds)\n",
    "    return i,g\n",
    "\n",
    "\n",
    "def run_iradon_id11(grain, pad=20, y0=0, workers=1, sample_mask=None, apply_halfmask=False, mask_central_zingers=False):\n",
    "    outsize = grain.ssino.shape[0] + pad\n",
    "    \n",
    "    if apply_halfmask:\n",
    "        halfmask = np.zeros_like(grain.ssino)\n",
    "\n",
    "        halfmask[:len(halfmask)//2-1, :] = 1\n",
    "        halfmask[len(halfmask)//2-1, :] = 0.5\n",
    "        \n",
    "        ssino_to_recon = grain.ssino * halfmask\n",
    "    else:\n",
    "        ssino_to_recon = grain.ssino\n",
    "        \n",
    "    # # pad the sample mask\n",
    "    # sample_mask_padded = np.pad(sample_mask, pad//2)\n",
    "\n",
    "    \n",
    "    # Perform iradon transform of grain sinogram, store result (reconstructed grain shape) in g.recon\n",
    "    grain.recon = ImageD11.sinograms.roi_iradon.iradon(ssino_to_recon, \n",
    "                                                       theta=grain.sinoangles, \n",
    "                                                       mask=sample_mask,\n",
    "                                                       output_size=outsize,\n",
    "                                                       projection_shifts=np.full(grain.ssino.shape, -y0),\n",
    "                                                       filter_name='hamming',\n",
    "                                                       interpolation='linear',\n",
    "                                                       workers=workers)\n",
    "    \n",
    "    if mask_central_zingers:\n",
    "        grs = grain.recon.shape[0]\n",
    "        xpr, ypr = -grs//2 + np.mgrid[:grs, :grs]\n",
    "        inner_mask_radius = 25\n",
    "        outer_mask_radius = inner_mask_radius + 2\n",
    "\n",
    "        inner_circle_mask = (xpr ** 2 + ypr ** 2) < inner_mask_radius ** 2\n",
    "        outer_circle_mask = (xpr ** 2 + ypr ** 2) < outer_mask_radius ** 2\n",
    "\n",
    "        mask_ring = inner_circle_mask & outer_circle_mask\n",
    "        # we now have a mask to apply\n",
    "        fill_value = np.median(grain.recon[mask_ring])\n",
    "        grain.recon[inner_circle_mask] = fill_value\n",
    "    \n",
    "    return grain\n",
    "\n",
    "cmp = {'compression':'gzip',\n",
    "       'compression_opts': 2,\n",
    "       'shuffle' : True }\n",
    "\n",
    "def save_array(grp, name, ary):\n",
    "    hds = grp.require_dataset(name, \n",
    "                              shape=ary.shape,\n",
    "                              dtype=ary.dtype,\n",
    "                              **cmp)\n",
    "    hds[:] = ary\n",
    "    return hds\n",
    "\n",
    "def save_grains_for_mlem(grains, ds, y0):\n",
    "    with h5py.File(ds.grainsfile, 'r+') as hout:\n",
    "        try:\n",
    "            grp = hout.create_group('peak_assignments')\n",
    "        except ValueError:\n",
    "            grp = hout['peak_assignments']\n",
    "\n",
    "        ds_gord = save_array( grp, 'gord', gord )\n",
    "        ds_gord.attrs['description'] = 'Grain ordering: g[i].pks = gord[ inds[i] : inds[i+1] ]'\n",
    "        ds_inds = save_array( grp, 'inds', inds )\n",
    "        ds_inds.attrs['description'] = 'Grain indices: g[i].pks = gord[ inds[i] : inds[i+1] ]'\n",
    "        \n",
    "        grains_group = 'grains'\n",
    "        for g in tqdm(grains):\n",
    "            gg = hout[grains_group][str(g.gid)]\n",
    "            # save stuff for sinograms\n",
    "            \n",
    "            save_array(gg, 'ssino', g.ssino).attrs['description'] = 'Sinogram of peak intensities sorted by omega'\n",
    "            save_array(gg, 'sinoangles', g.sinoangles).attrs['description'] = 'Projection angles for sinogram'\n",
    "            save_array(gg, 'og_recon', g.og_recon).attrs['description'] = 'Original ID11 iRadon reconstruction'\n",
    "            save_array(gg, 'circle_mask', whole_sample_mask).attrs['description'] = 'Reconstruction mask to use for MLEM'\n",
    "            \n",
    "            # might as well save peaks stuff while we're here\n",
    "            save_array(gg, 'translation', g.translation).attrs['description'] = 'Grain translation in lab frame'\n",
    "            save_array(gg, 'peaks_2d_sinograms', g.peaks_2d).attrs['description'] = \"2D peaks from strong 4D peaks that were assigned to this grain for sinograms\"\n",
    "            save_array(gg, 'peaks_4d_sinograms', g.peaks_4d).attrs['description'] = \"Strong 4D peaks that were assigned to this grain for sinograms\"\n",
    "\n",
    "            gg.attrs['cen'] = g.cen\n",
    "            gg.attrs['y0'] = y0\n",
    "            \n",
    "            \n",
    "def prepare_mlem_bash(ds, grains, pad, is_half_scan, n_simultaneous_jobs=50, cores_per_task=8, niter=50):\n",
    "    \n",
    "    slurm_mlem_path = os.path.join(ds.analysispath, \"slurm_mlem\")\n",
    "\n",
    "    if os.path.exists(slurm_mlem_path):\n",
    "        print(f\"Removing {slurm_mlem_path}\")\n",
    "        rmtree(slurm_mlem_path)\n",
    "\n",
    "    os.mkdir(slurm_mlem_path)\n",
    "    \n",
    "    recons_path = os.path.join(ds.analysispath, \"mlem_recons\")\n",
    "\n",
    "    if os.path.exists(recons_path):\n",
    "        print(f\"Removing {recons_path}\")\n",
    "        rmtree(recons_path)\n",
    "\n",
    "    os.mkdir(recons_path)\n",
    "    \n",
    "    if is_half_scan:\n",
    "        dohm = \"Yes\"\n",
    "        mask_cen = \"Yes\"\n",
    "    else:\n",
    "        dohm = \"No\"\n",
    "        mask_cen = \"No\"\n",
    "    \n",
    "    bash_script_path = os.path.join(slurm_mlem_path, ds.dsname + '_mlem_recon_slurm.sh')\n",
    "    python_script_path = os.path.join(id11_code_path, \"ImageD11/nbGui/S3DXRD/run_mlem_recon.py\") \n",
    "    outfile_path =  os.path.join(slurm_mlem_path, ds.dsname + '_mlem_recon_slurm_%A_%a.out')\n",
    "    errfile_path =  os.path.join(slurm_mlem_path, ds.dsname + '_mlem_recon_slurm_%A_%a.err')\n",
    "    log_path = os.path.join(slurm_mlem_path, ds.dsname + '_mlem_recon_slurm_$SLURM_ARRAY_JOB_ID_$SLURM_ARRAY_TASK_ID.log')\n",
    "\n",
    "    reconfile = os.path.join(recons_path, ds.dsname + \"_mlem_recon_$SLURM_ARRAY_TASK_ID.txt\")\n",
    "\n",
    "    bash_script_string = f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name=mlem-recon\n",
    "#SBATCH --output={outfile_path}\n",
    "#SBATCH --error={errfile_path}\n",
    "#SBATCH --array=0-{len(grains)-1}%{n_simultaneous_jobs}\n",
    "#SBATCH --time=02:00:00\n",
    "# define memory needs and number of tasks for each array job\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task={cores_per_task}\n",
    "#\n",
    "date\n",
    "echo python3 {python_script_path} {ds.grainsfile} $SLURM_ARRAY_TASK_ID {reconfile} {pad} {niter} {dohm} {mask_cen} > {log_path} 2>&1\n",
    "python3 {python_script_path} {ds.grainsfile} $SLURM_ARRAY_TASK_ID {reconfile} {pad} {niter} {dohm} {mask_cen} > {log_path} 2>&1\n",
    "date\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(bash_script_path, \"w\") as bashscriptfile:\n",
    "        bashscriptfile.writelines(bash_script_string)\n",
    "        \n",
    "    return bash_script_path, recons_path\n",
    "\n",
    "\n",
    "def save_grains(grains, ds):\n",
    "    with h5py.File(ds.grainsfile, 'r+') as hout:\n",
    "        try:\n",
    "            grp = hout.create_group('slice_recon')\n",
    "        except ValueError:\n",
    "            grp = hout['slice_recon']\n",
    "        save_array(grp, 'intensity', raw_intensity_array).attrs['description'] = 'Raw intensity array for all grains'\n",
    "        save_array(grp, 'labels', grain_labels_array).attrs['description'] = 'Grain labels array for all grains'\n",
    "        \n",
    "        ipfxdset = save_array(grp, 'ipf_x_col_map', rgb_x_array)\n",
    "        ipfxdset.attrs['description'] = 'IPF X color at each pixel'\n",
    "        ipfxdset.attrs['CLASS'] = 'IMAGE'\n",
    "        ipfydset = save_array(grp, 'ipf_y_col_map', rgb_y_array)\n",
    "        ipfydset.attrs['description'] = 'IPF Y color at each pixel'\n",
    "        ipfydset.attrs['CLASS'] = 'IMAGE'\n",
    "        ipfzdset = save_array(grp, 'ipf_z_col_map', rgb_z_array)\n",
    "        ipfzdset.attrs['description'] = 'IPF Z color at each pixel'\n",
    "        ipfzdset.attrs['CLASS'] = 'IMAGE'\n",
    "        \n",
    "        grains_group = 'grains'\n",
    "\n",
    "        for g in tqdm(grains):\n",
    "            gg = hout[grains_group][str(g.gid)]\n",
    "            \n",
    "            save_array(gg, 'recon', g.recon).attrs['description'] = 'Final reconstruction'\n",
    "            \n",
    "            \n",
    "# without a mask, MLEM can introduce artifacts in the corners\n",
    "# so we can manually mask those out\n",
    "\n",
    "# we can incoporate our own mask too\n",
    "# by modifying the below function\n",
    "\n",
    "def apply_manual_mask(mask_in):\n",
    "    mask_out = mask_in.copy()\n",
    "    \n",
    "    # mask_out[200:, 250:] = 0\n",
    "    \n",
    "    return mask_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: For old datasets before the new directory layout structure, we don't distinguish between RAW_DATA and PROCESSED_DATA\n",
    "\n",
    "### USER: specify your experimental directory\n",
    "\n",
    "rawdata_path = \"/home/esrf/james1997a/Data/ihma439/id11/20231211/RAW_DATA\"\n",
    "\n",
    "!ls -lrt {rawdata_path}\n",
    "\n",
    "### USER: specify where you want your processed data to go\n",
    "\n",
    "processed_data_root_dir = \"/home/esrf/james1997a/Data/ihma439/id11/20231211/PROCESSED_DATA/James/20240226\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# USER: pick a sample and a dataset you want to segment\n",
    "\n",
    "sample = \"FeAu_0p5_tR_nscope\"\n",
    "dataset = \"top_250um\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# desination of H5 files\n",
    "\n",
    "dset_path = os.path.join(processed_data_root_dir, sample, f\"{sample}_{dataset}\", f\"{sample}_{dataset}_dataset.h5\")\n",
    "\n",
    "par_path = os.path.join(processed_data_root_dir, 'Fe_refined.par')\n",
    "\n",
    "e2dx_path = os.path.join(processed_data_root_dir, '../../CeO2/e2dx_E-08-0173_20231127.edf')\n",
    "e2dy_path = os.path.join(processed_data_root_dir, '../../CeO2/e2dy_E-08-0173_20231127.edf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the dataset (for motor positions, not sure why these are not in peaks)\n",
    "ds = ImageD11.sinograms.dataset.load(dset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import 4D peaks\n",
    "\n",
    "cf_4d = ImageD11.columnfile.columnfile(ds.col4dfile)\n",
    "\n",
    "cf_4d.parameters.loadparameters(par_path)\n",
    "cf_4d.updateGeometry()\n",
    "\n",
    "print(f\"Read {cf_4d.nrows} 4D peaks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grains = read_grains(ds)\n",
    "\n",
    "for grain in grains:\n",
    "    # print(grain.gid)\n",
    "    grain.a = np.cbrt(np.linalg.det(grain.ubi))\n",
    "    \n",
    "print(f\"{len(grains)} grains imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here we are filtering our peaks (cf_4d) to select only the strongest ones\n",
    "# this time as opposed to indexing, our frac is slightly weaker but we are NOT filtering in dstar!!!!!\n",
    "# this means many more peaks per grain = stronger sinograms\n",
    "\n",
    "# USER: modify the \"frac\" parameter below and re-run the cell until the orange dot sits nicely on the \"elbow\" of the blue line\n",
    "# this indicates the fractional intensity cutoff we will select\n",
    "# if the blue line does not look elbow-shaped in the logscale plot, try changing the \"doplot\" parameter (the y scale of the logscale plot) until it does\n",
    "\n",
    "cf_strong = utils.selectpeaks(cf_4d, frac=0.995, dsmax=cf_4d.ds.max(), doplot=0.9)\n",
    "print(cf_4d.nrows)\n",
    "cf_strong.nrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # now let's do a whole-sample tomographic reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If the sinograms are only half-sinograms (we scanned dty across half the sample rather than the full sample), set the below to true:\n",
    "is_half_scan = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_half_scan:\n",
    "    utils.correct_half_scan(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.assign_peaks_to_grains(grains, cf_strong, tol=0.25)\n",
    "\n",
    "print(\"Storing peak data in grains\")\n",
    "# iterate through all the grains\n",
    "for g in tqdm(grains):\n",
    "    # store this grain's peak indices so we know which 4D peaks we used for indexing\n",
    "    g.mask_4d = cf_strong.grain_id == g.gid\n",
    "    g.peaks_4d = cf_strong.index[cf_strong.grain_id == g.gid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for grain in grains:\n",
    "    # grain.peaks_4d_selected, grain.cen, grain.dx, grain.dy = utils.graincen(grain.gid, cf_strong, doplot=True, nsigma=1)\n",
    "    grain.rgb_z = utils.grain_to_rgb(grain, ax=(0,0,1),)# symmetry = Symmetry.cubic)\n",
    "    grain.rgb_y = utils.grain_to_rgb(grain, ax=(0,1,0),)# symmetry = Symmetry.cubic)\n",
    "    grain.rgb_x = utils.grain_to_rgb(grain, ax=(1,0,0),)# symmetry = Symmetry.cubic)\n",
    "    utils.fit_grain_position_from_sino(grain, cf_strong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make sure we get cen right (centre of rotation should be the middle of dty)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([g.cen for g in grains])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c0 = np.median([g.cen for g in grains])\n",
    "\n",
    "print('Center of rotation in dty', c0)\n",
    "\n",
    "y0 = c0/2\n",
    "\n",
    "print('y0 is', y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate sinogram for whole sample\n",
    "\n",
    "whole_sample_sino, xedges, yedges = np.histogram2d(cf_4d.dty, cf_4d.omega, bins=[ds.ybinedges, ds.obinedges])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(whole_sample_sino, interpolation=\"nearest\", vmin=0)\n",
    "ax.set_aspect(4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"quick\" whole-sample reconstruction\n",
    "\n",
    "pad = 50\n",
    "\n",
    "outsize = whole_sample_sino.shape[0] + pad\n",
    "\n",
    "nthreads = len(os.sched_getaffinity(os.getpid()))\n",
    "\n",
    "if is_half_scan:\n",
    "    halfmask = np.zeros_like(whole_sample_sino)\n",
    "\n",
    "    halfmask[:len(halfmask)//2-1, :] = 1\n",
    "    halfmask[len(halfmask)//2-1, :] = 0.5\n",
    "\n",
    "    ssino_to_recon = whole_sample_sino * halfmask\n",
    "else:\n",
    "    ssino_to_recon = whole_sample_sino\n",
    "\n",
    "recon = ImageD11.sinograms.roi_iradon.iradon(ssino_to_recon, \n",
    "                                           theta=ds.obincens, \n",
    "                                           output_size=outsize,\n",
    "                                           projection_shifts=np.full(whole_sample_sino.shape, -y0),\n",
    "                                           filter_name='hamming',\n",
    "                                           interpolation='linear',\n",
    "                                           workers=nthreads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we should be able to easily segment this using scikit-image\n",
    "recon_man_mask = apply_manual_mask(recon)\n",
    "\n",
    "thresh = threshold_otsu(recon_man_mask)\n",
    "\n",
    "# we can also override the threshold if we don't like it:\n",
    "\n",
    "# thresh = 0.05\n",
    "\n",
    "binary = recon_man_mask > thresh\n",
    "\n",
    "chull = convex_hull_image(binary)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, sharex=True, sharey=True, constrained_layout=True)\n",
    "axs[0].imshow(recon_man_mask, vmin=0)\n",
    "axs[1].imshow(binary)\n",
    "axs[2].imshow(chull)\n",
    "\n",
    "axs[0].set_title(\"Reconstruction\")\n",
    "axs[1].set_title(\"Binarised threshold\")\n",
    "axs[2].set_title(\"Convex hull\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "whole_sample_mask = chull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "m = cf_strong.grain_id >= 0\n",
    "ax.scatter(cf_strong.omega[m], cf_strong.dty[m], c=cf_strong.grain_id[m])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_unit_cell_lengths = [grain.a for grain in grains]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(mean_unit_cell_lengths)\n",
    "ax.set_xlabel(\"Grain ID\")\n",
    "ax.set_ylabel(\"Unit cell length\")\n",
    "plt.show()\n",
    "\n",
    "a0 = np.median(mean_unit_cell_lengths)\n",
    "    \n",
    "print(a0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.plot_grain_sinograms(grains, cf_strong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.sum(grains[0].mask_4d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.style.use('dark_background')\n",
    "fig, ax = plt.subplots(2,2, figsize=(12,12))\n",
    "a = ax.ravel()\n",
    "x = [g.dx for g in grains]\n",
    "y = [g.dy for g in grains]\n",
    "s = [g.mask_4d.sum()/10 for g in grains]\n",
    "a[0].scatter(x, y, c=[g.rgb_z for g in grains], s=s)\n",
    "a[0].set(title='IPF color Z',  aspect='equal')\n",
    "a[1].scatter(x, y, c=[g.rgb_y for g in grains], s=s)\n",
    "a[1].set(title='IPF color Y', aspect='equal')\n",
    "a[2].scatter(x, y, c=[g.rgb_x for g in grains], s=s)\n",
    "a[2].set(title='IPF color X',  aspect='equal')\n",
    "a[3].scatter(x, y, c=s)\n",
    "a[3].set(title='Number of 4D peaks', aspect='equal')\n",
    "\n",
    "fig.supxlabel(\"Lab x\")\n",
    "fig.supylabel(\"Lab y\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# populate translations of grains\n",
    "for g in grains:\n",
    "    g.translation = np.array([g.dx, g.dy, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Big scary block\n",
    "# Must understand what this does!\n",
    "\n",
    "# Ensure cf is sorted by spot3d_id\n",
    "# NOTE: spot3d_id should be spot4d_id, because we have merged into 4D?\n",
    "assert (np.argsort(cf_strong.spot3d_id) == np.arange(cf_strong.nrows)).all()\n",
    "\n",
    "# load the 2d peak labelling output\n",
    "pks = ImageD11.sinograms.properties.pks_table.load(ds.pksfile)\n",
    "\n",
    "# Grab the 2d peak centroids\n",
    "p2d = pks.pk2d(ds.omega, ds.dty)\n",
    "\n",
    "# NOTE: These are not spatially corrected?!\n",
    "\n",
    "numba_order, numba_histo = utils.counting_sort(p2d['spot3d_id'])\n",
    "\n",
    "grain_2d_id = utils.palloc(p2d['spot3d_id'].shape, np.dtype(int))\n",
    "\n",
    "cleanid = cf_strong.grain_id.copy()\n",
    "\n",
    "utils.find_grain_id(cf_strong.spot3d_id, cleanid, p2d['spot3d_id'], grain_2d_id, numba_order)\n",
    "\n",
    "gord, counts = utils.counting_sort(grain_2d_id)\n",
    "\n",
    "inds = np.concatenate(((0,), np.cumsum(counts)))\n",
    "\n",
    "# I think what we end up with is:\n",
    "# inds\n",
    "# this is an array which tells you which 2D spots each grain owns\n",
    "# the 2D spots are sorted by spot ID\n",
    "# inds tells you for each grain were you can find its associated 2D spots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now our 2D peak assignments are known, let's populate our grain objects with our 2D peaks\n",
    "\n",
    "for grain in tqdm(grains):\n",
    "    i = grain.gid\n",
    "    grain.peaks_2d = gord[inds[i+1] : inds[i+2]]\n",
    "    # grain.mask_2d = np.isin(cf_2d.index, grain.peaks_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Determine sinograms of all grains\n",
    "\n",
    "nthreads = len(os.sched_getaffinity(os.getpid()))\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers= max(1,nthreads-1)) as pool:\n",
    "    for i in tqdm(pool.map(do_sinos, grains), total=len(grains)):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show sinogram of single grain\n",
    "\n",
    "g = grains[0]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.imshow((g.ssino/g.ssino.mean(axis=0)), norm=matplotlib.colors.LogNorm(), interpolation='nearest', origin=\"lower\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if you want, you can override the y0 value here\n",
    "\n",
    "# y0 = 1.5  # for example!\n",
    "\n",
    "y0 = c0/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = grains[1]\n",
    "\n",
    "run_iradon_id11(g, pad=pad, y0=y0, workers=max(nthreads, 20), sample_mask=whole_sample_mask, apply_halfmask=is_half_scan, mask_central_zingers=is_half_scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = grains[1]\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(10,5))\n",
    "axs[0].imshow(g.recon, vmin=0)\n",
    "axs[0].set_title(\"ID11 iradon\")\n",
    "axs[1].imshow(g.ssino, aspect='auto')\n",
    "axs[1].set_title(\"ssino\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nthreads = len(os.sched_getaffinity(os.getpid()))\n",
    "\n",
    "run_this_iradon = partial(run_iradon_id11, pad=pad, y0=y0, sample_mask=whole_sample_mask, workers=1, apply_halfmask=is_half_scan, mask_central_zingers=is_half_scan)\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor( max_workers= max(1,nthreads-1) ) as pool:\n",
    "    for i in tqdm(pool.map(run_this_iradon, grains), total=len(grains)):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for grain in grains:\n",
    "    grain.og_recon = grain.recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, a = plt.subplots(1,2,figsize=(10,5))\n",
    "rec = a[0].imshow(grains[8].og_recon, vmin=0)\n",
    "sin = a[1].imshow(grains[8].ssino, aspect='auto')\n",
    "\n",
    "# Function to update the displayed image based on the selected frame\n",
    "def update_frame(i):\n",
    "    rec.set_array(grains[i].og_recon)\n",
    "    sin.set_array(grains[i].ssino)\n",
    "    a[0].set(title=str(i))\n",
    "    fig.canvas.draw()\n",
    "\n",
    "# Create a slider widget to select the frame number\n",
    "frame_slider = widgets.IntSlider(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=len(grains) - 1,\n",
    "    step=1,\n",
    "    description='Grain:'\n",
    ")\n",
    "\n",
    "interact(update_frame, i=frame_slider)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f,a = plt.subplots( 1,3, figsize=(15,5) )\n",
    "ty, tx = utils.triangle().T\n",
    "for i,title in enumerate( 'xyz' ):\n",
    "    ax = np.zeros(3)\n",
    "    ax[i] = 1.\n",
    "    hkl = [utils.crystal_direction_cubic( g.ubi, ax ) for g in grains]\n",
    "    xy = np.array([utils.hkl_to_pf_cubic(h) for h in hkl ])\n",
    "    rgb = np.array([utils.hkl_to_color_cubic(h) for h in hkl ])\n",
    "    for j in range(len(grains)):\n",
    "        grains[j].rgb = rgb[j]\n",
    "    a[i].scatter( xy[:,1], xy[:,0], c = rgb )   # Note the \"x\" axis of the plot is the 'k' direction and 'y' is h (smaller)\n",
    "    a[i].set(title=title, aspect='equal', facecolor='silver', xticks=[], yticks=[])\n",
    "    a[i].plot( tx, ty, 'k-', lw = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rgb_x_array, rgb_y_array, rgb_z_array, grain_labels_array, raw_intensity_array = utils.build_slice_arrays(grains, cutoff_level=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot initial output\n",
    "\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.imshow(rgb_z_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.imshow(grain_labels_array)  # originally 1,2,0\n",
    "ax.set_title(\"Grain label map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.imshow(raw_intensity_array)\n",
    "ax.set_title(\"Raw intensity array\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can clean up these reconstructions using an MLEM iterative recon\n",
    "# we can use the whole sample shape mask for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_grains_for_mlem(grains, ds, y0=y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bash_script_path, recons_path = prepare_mlem_bash(ds, grains, pad, is_half_scan, n_simultaneous_jobs=50, cores_per_task=8, niter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.slurm_submit_and_wait(bash_script_path, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# collect results into grain attributes\n",
    "# the filenames are element position not gid\n",
    "\n",
    "for i, grain in enumerate(tqdm(grains)):\n",
    "    grain.recon = np.loadtxt(os.path.join(recons_path, ds.dsname + f\"_mlem_recon_{i}.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# look at all our grains\n",
    "\n",
    "n_grains_to_plot = 25\n",
    "\n",
    "grains_step = len(grains)//n_grains_to_plot\n",
    "\n",
    "grid_size = np.ceil(np.sqrt(len(grains[::grains_step]))).astype(int)\n",
    "nrows = (len(grains[::grains_step])+grid_size-1)//grid_size\n",
    "\n",
    "fig, axs = plt.subplots(grid_size, nrows, figsize=(10,10), layout=\"constrained\", sharex=True, sharey=True)\n",
    "for i, ax in enumerate(axs.ravel()):\n",
    "    if i < len(grains[::grains_step]):\n",
    "    # get corresponding grain for this axis\n",
    "        g = grains[::grains_step][i]\n",
    "        ax.imshow(g.recon, vmin=0)\n",
    "        # ax.invert_yaxis()\n",
    "        ax.set_title(i)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rgb_x_array, rgb_y_array, rgb_z_array, grain_labels_array, raw_intensity_array = utils.build_slice_arrays(grains, cutoff_level=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.imshow(rgb_z_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.imshow(raw_intensity_array)\n",
    "ax.set_title(\"Sinogram raw intensity map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.imshow(grain_labels_array)\n",
    "ax.set_title(\"Grain label map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_grains(grains, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    raise ValueError(\"Change the 1 above to 0 to allow 'Run all cells' in the notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now that we're happy with our segmentation parameters, we can run the below cell to do this in bulk for many samples/datasets\n",
    "# by default this will do all samples in sample_list, all datasets with a prefix of dset_prefix\n",
    "# you can add samples and datasets to skip\n",
    "\n",
    "skips_dict = {\n",
    "    \"FeAu_0p5_tR_nscope\": [\"top_-50um\", \"top_-100um\"]\n",
    "}\n",
    "\n",
    "dset_prefix = \"top\"\n",
    "\n",
    "sample_list = [\"FeAu_0p5_tR_nscope\"]\n",
    "    \n",
    "samples_dict = utils.find_datasets_to_process(rawdata_path, skips_dict, dset_prefix, sample_list)\n",
    "    \n",
    "# manual override:\n",
    "# samples_dict = {\"FeAu_0p5_tR_nscope\": [\"top_100um\", \"top_200um\"]}\n",
    "    \n",
    "# now we have our samples_dict, we can process our data:\n",
    "\n",
    "par_path = os.path.join(processed_data_root_dir, 'Fe_refined.par')\n",
    "\n",
    "e2dx_path = os.path.join(processed_data_root_dir, '../../CeO2/e2dx_E-08-0173_20231127.edf')\n",
    "e2dy_path = os.path.join(processed_data_root_dir, '../../CeO2/e2dy_E-08-0173_20231127.edf')\n",
    "\n",
    "cf_strong_frac = 0.995\n",
    "cf_strong_dstol = 0.01\n",
    "\n",
    "is_half_scan = False\n",
    "\n",
    "peak_assign_tol = 0.25\n",
    "\n",
    "manual_threshold = None\n",
    "# manual_threshold = 0.025\n",
    "\n",
    "nthreads = len(os.sched_getaffinity(os.getpid()))\n",
    "\n",
    "pad = 50\n",
    "\n",
    "# y0 = -1.4\n",
    "\n",
    "mlem_n_simultaneous_jobs = 50\n",
    "mlem_cores_per_task = 8\n",
    "mlem_niter = 50\n",
    "\n",
    "cutoff_level = 0.2\n",
    "\n",
    "for sample, datasets in samples_dict.items():\n",
    "    for dataset in datasets:\n",
    "        print(f\"Processing dataset {dataset} in sample {sample}\")\n",
    "        dset_path = os.path.join(processed_data_root_dir, sample, f\"{sample}_{dataset}\", f\"{sample}_{dataset}_dataset.h5\")\n",
    "        if not os.path.exists(dset_path):\n",
    "            print(f\"Missing DataSet file for {dataset} in sample {sample}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        print(\"Importing DataSet object\")\n",
    "        \n",
    "        ds = ImageD11.sinograms.dataset.load(dset_path)\n",
    "        print(f\"I have a DataSet {ds.dset} in sample {ds.sample}\")\n",
    "        \n",
    "        if not os.path.exists(ds.grainsfile):\n",
    "            print(f\"Missing grains file for {dataset} in sample {sample}, skipping\")\n",
    "            continue\n",
    "            \n",
    "        # check grains file for existance of slice_recon, skip if it's there\n",
    "        with h5py.File(ds.grainsfile, \"r\") as hin:\n",
    "            if \"slice_recon\" in hin.keys():\n",
    "                print(f\"Already reconstructed {dataset} in {sample}, skipping\")\n",
    "                continue\n",
    "            \n",
    "        cf_4d = ImageD11.columnfile.columnfile(ds.col4dfile)\n",
    "        cf_4d.parameters.loadparameters(par_path)\n",
    "        cf_4d.updateGeometry()\n",
    "        \n",
    "        grains = read_grains(ds)\n",
    "        \n",
    "        cf_strong = utils.selectpeaks(cf_4d, frac=cf_strong_frac, dsmax=cf_4d.ds.max(), dstol=cf_strong_dstol)\n",
    "        \n",
    "        if is_half_scan:\n",
    "            utils.correct_half_scan(ds)\n",
    "            \n",
    "        utils.assign_peaks_to_grains(grains, cf_strong, tol=peak_assign_tol)\n",
    "        \n",
    "        for g in tqdm(grains):\n",
    "            g.mask_4d = cf_strong.grain_id == g.gid\n",
    "            g.peaks_4d = cf_strong.index[cf_strong.grain_id == g.gid]\n",
    "            \n",
    "        for grain in tqdm(grains):\n",
    "            # grain.peaks_4d_selected, grain.cen, grain.dx, grain.dy = utils.graincen(grain.gid, cf_strong, doplot=False)\n",
    "            grain.rgb_z = utils.grain_to_rgb(grain, ax=(0,0,1),)# symmetry = Symmetry.cubic)\n",
    "            grain.rgb_y = utils.grain_to_rgb(grain, ax=(0,1,0),)# symmetry = Symmetry.cubic)\n",
    "            grain.rgb_x = utils.grain_to_rgb(grain, ax=(1,0,0),)# symmetry = Symmetry.cubic)\n",
    "            utils.fit_grain_position_from_sino(grain, cf_strong)\n",
    "            \n",
    "        c0 = np.median([g.cen for g in grains])\n",
    "        \n",
    "        y0 = c0/2\n",
    "        \n",
    "        whole_sample_sino, xedges, yedges = np.histogram2d(cf_4d.dty, cf_4d.omega, bins=[ds.ybinedges, ds.obinedges])\n",
    "        \n",
    "        print(\"Whole sample mask\")\n",
    "        outsize = whole_sample_sino.shape[0] + pad\n",
    "\n",
    "        if is_half_scan:\n",
    "            halfmask = np.zeros_like(whole_sample_sino)\n",
    "\n",
    "            halfmask[:len(halfmask)//2-1, :] = 1\n",
    "            halfmask[len(halfmask)//2-1, :] = 0.5\n",
    "\n",
    "            ssino_to_recon = whole_sample_sino * halfmask\n",
    "        else:\n",
    "            ssino_to_recon = whole_sample_sino\n",
    "        \n",
    "        recon = ImageD11.sinograms.roi_iradon.iradon(ssino_to_recon, \n",
    "                                           theta=ds.obincens, \n",
    "                                           output_size=outsize,\n",
    "                                           projection_shifts=np.full(whole_sample_sino.shape, -y0),\n",
    "                                           filter_name='hamming',\n",
    "                                           interpolation='linear',\n",
    "                                           workers=nthreads)\n",
    "        \n",
    "        recon_man_mask = apply_manual_mask(recon)\n",
    "        if manual_threshold is None:\n",
    "            thresh = threshold_otsu(recon_man_mask)\n",
    "        else:\n",
    "            thresh = manual_threshold\n",
    "            \n",
    "        binary = recon_man_mask > thresh\n",
    "        whole_sample_mask = convex_hull_image(binary)\n",
    "        \n",
    "        for g in grains:\n",
    "            g.translation = np.array([g.dx, g.dy, 0])\n",
    "        \n",
    "        print(\"Peak 2D organise\")\n",
    "        pks = ImageD11.sinograms.properties.pks_table.load(ds.pksfile)\n",
    "        p2d = pks.pk2d(ds.omega, ds.dty)\n",
    "        numba_order, numba_histo = utils.counting_sort(p2d['spot3d_id'])\n",
    "        grain_2d_id = utils.palloc(p2d['spot3d_id'].shape, np.dtype(int))\n",
    "        cleanid = cf_strong.grain_id.copy()\n",
    "        utils.find_grain_id(cf_strong.spot3d_id, cleanid, p2d['spot3d_id'], grain_2d_id, numba_order)\n",
    "        gord, counts = utils.counting_sort(grain_2d_id)\n",
    "        inds = np.concatenate(((0,), np.cumsum(counts)))\n",
    "        \n",
    "        for grain in tqdm(grains):\n",
    "            i = grain.gid\n",
    "            grain.peaks_2d = gord[inds[i+1] : inds[i+2]]\n",
    "        \n",
    "        print(\"Making sinograms\")\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers= max(1,nthreads-1)) as pool:\n",
    "            for i in tqdm(pool.map(do_sinos, grains), total=len(grains)):\n",
    "                pass\n",
    "        \n",
    "        print(\"Running iradon\")\n",
    "        \n",
    "        run_this_iradon = partial(run_iradon_id11, pad=pad, y0=y0, sample_mask=whole_sample_mask, workers=1, apply_halfmask=is_half_scan, mask_central_zingers=is_half_scan)\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor( max_workers= max(1,nthreads-1) ) as pool:\n",
    "            for i in tqdm(pool.map(run_this_iradon, grains), total=len(grains)):\n",
    "                pass\n",
    "            \n",
    "        for grain in grains:\n",
    "            grain.og_recon = grain.recon\n",
    "            \n",
    "        save_grains_for_mlem(grains, ds, y0)\n",
    "        \n",
    "        bash_script_path, recons_path = prepare_mlem_bash(ds, grains, pad, is_half_scan, mlem_n_simultaneous_jobs, mlem_cores_per_task, mlem_niter)\n",
    "        \n",
    "        utils.slurm_submit_and_wait(bash_script_path, 30)\n",
    "        \n",
    "        for i, grain in enumerate(tqdm(grains)):\n",
    "            grain.recon = np.loadtxt(os.path.join(recons_path, ds.dsname + f\"_mlem_recon_{i}.txt\"))\n",
    "            \n",
    "        rgb_x_array, rgb_y_array, rgb_z_array, grain_labels_array, raw_intensity_array = utils.build_slice_arrays(grains, cutoff_level)\n",
    "        \n",
    "        save_grains(grains, ds)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (main)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
