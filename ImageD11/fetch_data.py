# coding: utf-8

import os

import six
from six.moves import urllib  # python 2/3 compatible

import ImageD11.sinograms.dataset as id11dset

if six.PY2:  # python 2/3 compatibility
    FileNotFoundError = IOError

"""
A list of functions used to import test datasets from Zenodo for CI/Jupyter notebooks.
Inspired by orix/data/__init__.py from https://github.com/pyxem/orix
"""

# what Zenodo folder can these be found in?
dataset_base_urls = {
    'Si_cube_S3DXRD_nt_moves_dty': "https://sandbox.zenodo.org/records/118843/files/",
}

# What are the names of the files in the Zenodo folder?
dataset_filenames = {
    'Si_cube_S3DXRD_nt_moves_dty': {
        'sparsefile': 'Si_cube_S3DXRD_nt_moves_dty_sparse.h5',
        'parfile': 'pars.json',
        'geomfile': 'geometry.par',
        'phasefile0': 'Si_refined.par',
        'e2dxfile': 'e2dx_E-08-0144_20240205.edf',
        'e2dyfile': 'e2dy_E-08-0144_20240205.edf'
    }
}

# Also what's the sample and what's the dataset name?
dataset_metadata = {
    'Si_cube_S3DXRD_nt_moves_dty': {
        'sample': 'Si_cube',
        'dataset': 'S3DXRD_nt_moves_dty',
    }
}


def download_url(url, dest_file_path):
    """Download a URl using urllib"""
    print('Downloading ' + url + ' to ' + dest_file_path)
    urllib.request.urlretrieve(url, dest_file_path)
    if not os.path.exists(dest_file_path):
        raise FileNotFoundError('download failed!')


def _get_dataset(test_dataset_name, dest_folder, allow_download):
    """Get a dataset from disk or Zenodo given the dataset name and a destination folder"""
    if test_dataset_name not in dataset_metadata.keys():
        raise ValueError('Invalid dataset name supplied!')
    if test_dataset_name not in dataset_filenames.keys():
        raise ValueError('Invalid dataset name supplied!')
    if test_dataset_name not in dataset_base_urls.keys():
        raise ValueError('Invalid dataset name supplied!')

    absolute_path = os.path.abspath(dest_folder)

    if not os.path.exists(absolute_path):
        raise FileNotFoundError('dest_folder path does not exist!')
    dest_folder = absolute_path
    # Destination folder exists, so we continue
    # See if there's a dataset in the folder first

    # Try to find the dataset locally
    # Do we have a dataset H5 already?
    sample = dataset_metadata[test_dataset_name]['sample']
    dsname = dataset_metadata[test_dataset_name]['dataset']

    raw_data_root_dir = os.path.join(dest_folder, 'raw')
    processed_data_root_dir = os.path.join(dest_folder, 'processed')

    # at this point, we have enough information to make a dataset object
    # that way, we can use the paths generated by the dataset object
    # and we don't need to redeclare the path structure for our processed data
    ds = id11dset.DataSet(dataroot=raw_data_root_dir,
                          analysisroot=processed_data_root_dir,
                          sample=sample,
                          dset=dsname)

    # if the dsfile path exists, the init method for ds should have populated everything, so return:

    if os.path.exists(ds.dsfile):
        # ds.dsfile exists!
        # replace ds with one imported directly from file
        # so we get the extra attributes
        ds = id11dset.load(ds.dsfile)

        print('Already found a dataset downloaded! Returning that')
        return ds

    if os.path.exists(ds.sparsefile):
        print('Found a sparse file downloaded! Making a dataset to return...')
        # we have a sparse h5 path
        ds.import_from_sparse(ds.sparsefile)

        # do we also have spatials and a parfile?

        for filetype, filename in dataset_filenames[test_dataset_name].items():
            # is it a file that could be used as an attribute?
            if filetype in id11dset.DataSet.ATTRNAMES:
                if filetype != "sparsefile":
                    # spatial or par
                    # should be in processed_data_root_dir
                    filepath = os.path.join(processed_data_root_dir, filename)
                    if os.path.exists(filepath):
                        setattr(ds, filetype, filepath)

        ds.save()
        return ds

    if allow_download:
        print('Downloading files!')
        # We don't have local files
        # We're allowed to download them
        # make a raw and processed folder

        os.mkdir(raw_data_root_dir)
        os.makedirs(ds.analysispath)

        for filetype, filename in dataset_filenames[test_dataset_name].items():
            file_url = dataset_base_urls[test_dataset_name] + filename
            # is it a file that could be used as an attribute?
            # if filetype in id11dset.DataSet.ATTRNAMES:
            if hasattr(ds, filetype):
                if getattr(ds, filetype) is None:
                    # this is an attribute, but we don't have a path for it
                    # put it in processed data root
                    filepath = os.path.join(processed_data_root_dir, filename)
                    download_url(file_url, filepath)
                    setattr(ds, filetype, filepath)
                else:
                    # the dataset has a path for this filetype already
                    filepath = getattr(ds, filetype)
                    download_url(file_url, filepath)
            else:
                # probably a spatial or a parfile
                # chuck it in processed_data_root_dir
                # set the attribute
                filepath = os.path.join(processed_data_root_dir, filename)
                download_url(file_url, filepath)
                setattr(ds, filetype, filepath)

        ds.import_from_sparse(ds.sparsefile)
        ds.save()
        return ds
    else:
        print("Couldn't find the files on the disk and allow_download is False!")


def si_cube_s3dxrd_dataset(dest_folder, allow_download=False):
    return _get_dataset('Si_cube_S3DXRD_nt_moves_dty', dest_folder=dest_folder, allow_download=allow_download)
